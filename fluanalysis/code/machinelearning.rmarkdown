---
title: "Flu Anlaysis - Machine Learning"
output:
  html_document:
    toc: FALSE
editor: 
  markdown: 
    wrap: 72
---


Focused on single outcome, the continuous, numerical value of Body
Temperature.

## Load Libraries


```{r echo = FALSE}
# Load Libraries
library(tidyverse) #data exploration packages
library(dplyr) #piping
library(here) #data location
library(vip)
library(rpart.plot) 
library(tidymodels)
library(glmnet)
library(ranger)
```


## Load the data


```{r}
#path to clean data
data = readRDS(here("fluanalysis", "data", "cleandata2.rds")) #load RDS file
```


## Check the data


```{r}
#check the data to make sure it has loaded properly
head(data)
```


#Setup

1.  Set random seed to 123
2.  Split the dataset into 70% training 30% testing. Use BodyTemp as
    stratification
3.  Do 5-fold cross-validation, 5 times repeated.
4.  Create a recipe for the data and fitting

## 1. Set random seed to 123


```{r}
# set seed 
set.seed(123)
```


## 2. Split the data


```{r}
## Split the data into test and training
data_split = initial_split(data,strata = BodyTemp, prop = 7/10)

#create training and test
data_train=training(data_split)
data_test=testing(data_split)
```


## 3. Cross validation


```{r}
# 5 fold cross-validation 5 times repeated 5x5
# stratify on Body Temp
# use vfold_cv to create a resample object for the training data 

#CV on training data
fold_train = vfold_cv(data_train, v = 5, repeats = 5, strata = BodyTemp)
fold_train

#CV on test data
fold_test = vfold_cv(data_test, v = 5, repeats = 5, strata = BodyTemp)
fold_test
```


## 4. Create a recipe for the data and fitting


```{r}
# categorical variables as dummy variables 
#pick all nominal predictor variables 

data_recipe = recipe(BodyTemp ~ ., data = data_train) %>%
  step_dummy(all_nominal(), -all_outcomes()) 

```


# Null Model Performance

### compute the performance of a null model


```{r}
# compute the performance of a null model
nullmodel = null_model() %>% 
  set_engine("parsnip") %>%
  set_mode("regression")
```


### Compute the RSME for training and test data for the model


```{r}

## training data

#recipe
nullr_train = recipe(BodyTemp ~ 1, data = data_train)

#workflow
nullw_train = workflow() %>%
  add_model(nullmodel) %>% 
  add_recipe(nullr_train)

#fit
nullf_train = fit_resamples(nullw_train, resamples = fold_train)



#null model recipe with testing data

#recipe
nullr_test = recipe(BodyTemp ~ 1, data = data_test)

#workflow
nullw_test = workflow() %>% 
  add_model(nullmodel) %>% 
  add_recipe(nullr_test)

#fit
nullf_test = fit_resamples(nullw_test, resamples = fold_test)

```


### Null Model Metrics


```{r}
#RMSE and RSQ for training set
nullf_train %>% collect_metrics()

#RMSE and RSQ for test set
nullf_test %>% collect_metrics()
```


# Model Tuning and Fitting

1.  Tree
2.  LASSO
3.  random forest

The steps (blocks of code) included are 1) model specification, 2)
workflow definition, 3) tuning grip specification and 4) tuning using
cross-validation and the tune_grip function

## Tree

### Model specification


```{r}
tree_spec = decision_tree(cost_complexity = tune(),tree_depth = tune())%>%
  set_engine("rpart")%>%
  set_mode("regression")

tree_spec
```


### Workflow definition


```{r}
#create workflow
tree_workflow = workflow()%>%
  add_model(tree_spec)%>%
  add_recipe(data_recipe) #recipe created in step 4 of the setup
```


### tuning grid specification


```{r}
tree_grid = grid_regular(cost_complexity(),
                         tree_depth(),
                         levels = 5)
tree_grid

#depth
tree_grid %>%
  count(tree_depth)
```


### tuning using cross validation


```{r}
tree_cv = tree_workflow %>%
  tune_grid(
    resamples = fold_train,
    grid = tree_grid
  )
```

```{r}
tree_cv
```

```{r}
#use collect metrics to give tibble with the results from the tuning
tree_cv %>%
  collect_metrics()
```


### Model Evaluation

Look at diagnostics using autoplot().


```{r}
tree_cv %>% autoplot()
```


Get the model that the tuning process has determined is the best using
select_best() and finalize_workflow().


```{r}
tree_cv %>%
  show_best(metric = "rmse")
```

```{r}
tree_best = tree_cv %>%
  select_best(metric = "rmse")
tree_best
```


Finalize workflow with the fit() function


```{r}
tree_f_workflow = tree_workflow %>%
  finalize_workflow(tree_best)

tree_f_fit = tree_f_workflow %>% fit(data=data_train)
tree_f_fit
```

```{r}
#plot tree
rpart.plot(extract_fit_parsnip(tree_f_fit)$fit)
```


#### evaluate the final fit


```{r}
#predicted and residuals
tree_residuals = tree_f_fit %>%
  augment(data_train) %>% #use augment() to make predictions from train data
  select(c(.pred, BodyTemp)) %>%
  mutate(.resid = BodyTemp - .pred) #calculate residuals and make new row.

tree_residuals
```

```{r}
# Plot predicted values vs actual values
plot_tree_predicted = tree_residuals %>%
  ggplot(aes(x = BodyTemp, y = .pred)) + 
  geom_point() + 
  labs(title = "Predicted outcomes vs Actual Outcomes", 
       x = "Body Temp Actual", 
       y = "Body Temp Prediction")
plot_tree_predicted
```

```{r}
# Plot predicted values vs residuals
plot_tree_residual = ggplot(tree_residuals, 
                              aes(y = .resid, 
                              x = .pred)) + 
  geom_point() + 
  labs(title = "Prediction Outcomes vs Residuals: Decision Tree", 
       x = "Body Temp Prediction", 
       y = "Residuals")
plot(plot_tree_residual) 
```

### performance 


```{r}
tree_cv %>%
  show_best(metric = "rmse", n=1)
```



## LASSO

### Model specification


```{r}
#mixture = 1 -> glmnet will remove irrelevant predictors
lasso_mod = linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```


### Workflow definition


```{r}
lasso_workflow = workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(data_recipe)
```


### tuning grid specification


```{r}
lasso_grid = tibble(penalty = 10^seq(-4, -1, length.out = 30))
lasso_grid %>% top_n(-6)
lasso_grid %>% top_n(6)
```


### tuning using cross validation


```{r}
lasso_cv = lasso_workflow %>%
  tune_grid(resamples = fold_train,
            grid = lasso_grid,
            control = control_grid(verbose = FALSE, save_pred = TRUE),
            metrics = metric_set(rmse))

lasso_cv%>% collect_metrics()
```


### Model Evaluation

Look at diagnostics using autoplot().


```{r}
lasso_cv %>% autoplot()
```


Get the model that the tuning process has determined is the best using
select_best() and finalize_workflow().


```{r}
lasso_cv %>%
  show_best(metric = "rmse")
```

```{r}
lasso_best = lasso_cv %>%
  select_best(metric = "rmse")
lasso_best
```


Finalize workflow with the fit() function


```{r}
lasso_f_workflow = lasso_workflow %>%
  finalize_workflow(lasso_best)

lasso_f_fit = lasso_f_workflow %>% fit(data=data_train)
lasso_f_fit
```

```{r}
#plot for how the number of predictors included in the LASSO model changes with the tuning parameter
x = lasso_f_fit$fit$fit$fit
plot(x, "lambda")
```


#### evaluate the final fit (repeat)


```{r}

lasso_residual = lasso_f_fit %>%
  augment(data_train) %>% 
  select(c(.pred, BodyTemp)) %>%
  mutate(resid = BodyTemp - .pred) 
lasso_residual
```

```{r}
# Plot predicted values vs actual values
plot_lasso_predicted = lasso_residual %>%
  ggplot(aes(x = BodyTemp, y = .pred)) + 
  geom_point() + 
  labs(title = "Predicted Outcomes vs Actual Outcomes", 
       x = "Body Temp Actual", 
       y = "Body Temp Prediction")
plot_lasso_predicted
```

```{r}
# Plot predicted values vs residuals
plot_lasso_residual = lasso_residual %>% 
  ggplot(aes(x = resid, y = .pred)) + 
  geom_point() +
  labs(title = "Predictions vs Residual", 
       x = "Residuals", 
       y = "Body Temp Prediction")
plot_lasso_residual
```

### performance 


```{r}
lasso_cv %>%
  show_best(metric = "rmse", n=1)
```


## Random Forest

### Model specification


```{r}
cores = parallel::detectCores()
cores

randomfor_model <-
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  set_engine("ranger", num.threads = cores) %>%
  set_mode("regression")
```


### Workflow definition


```{r}
randomfor_workflow <-
  workflow() %>%
  add_model(randomfor_model) %>%
  add_recipe(data_recipe)


```


### tuning grid specification


```{r}
randomfor_model
```

```{r}
extract_parameter_set_dials(randomfor_model)
```


### tuning using cross validation


```{r}
randomfor_cv = randomfor_workflow %>%
  tune_grid(resamples = fold_train,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))


```

```{r}
randomfor_cv %>% 
  collect_metrics()
```


### Model Evaluation

Look at diagnostics using autoplot().


```{r}
autoplot(randomfor_cv)
```


Get the model that the tuning process has determined is the best using
select_best() and finalize_workflow().


```{r}
randomfor_cv %>%
  show_best(metric = "rmse")
```

```{r}
randomfor_best = randomfor_cv %>%
  select_best(metric = "rmse")
randomfor_best
```


finalize workflow


```{r}
randomfor_f_workflow = randomfor_workflow %>%
  finalize_workflow(randomfor_best)

randomfor_f_fit = randomfor_f_workflow %>% fit(data=data_train)
randomfor_f_fit
```

#### evaluate the final fit (repeat)

```{r}
# get predicted and residual values in one dataset 
randomfor_residual = randomfor_f_fit %>%
  augment(data_train) %>% 
  select(c(.pred, BodyTemp)) %>%
  mutate(resid = BodyTemp - .pred) 
randomfor_residual
```

```{r}
# Plot actual values vs predicted values
plot_randomfor_predicted = randomfor_residual %>%
  ggplot(aes(x = BodyTemp, y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual", 
       x = "Body Temp Actual", 
       y = "Body Temp Prediction")
plot_randomfor_predicted
```

```{r}
# Plot predicted values vs residuals
plot_randomfor_residual = randomfor_residual %>% 
  ggplot(aes(x = resid, y = .pred)) + 
  geom_point() +
  labs(title = "Predictions vs Residual", 
       x = "Residual", 
       y = "Body Temp Prediction")
plot_randomfor_residual
```

### performance 


```{r}
randomfor_cv %>%
  show_best(metric = "rmse", n=1)
```


# Final Evaluation
All models have similar performance based on RMSE. When using the plots for visual analysis of the performance, you can see there is more of a visual relationship using LASSO and Random Forest but not the tree model. I selected the Lasso model because when accounting for more significant figures, it is lower.

```{r}
#fit the test data once 
lasso_lastfit = lasso_f_workflow %>%
  last_fit(data_split)

lasso_lastfit %>% collect_metrics()
```

```{r}
#compare with the null

nullf_test %>% collect_metrics()
```

```{r}
#lasso_last_residual = lasso_lastfit %>%
#  augment(data_test) %>% 
#  select(c(.pred, BodyTemp)) %>%
#  mutate(resid = BodyTemp - .pred) 
#lasso_last_residual
```



# Conclusion

The Data above show that the model with all the possible predictors is
the best model to fit the data as it minimizes the Root Mean Squared
Errors (RMSE) of the data.

